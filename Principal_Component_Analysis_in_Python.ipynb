{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis Essentials in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to optimize you machine learning models. A common way is to reduce the dimensionality of your dataset. You can do it via Feature Selection. A more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA). If your learning algorithm is too slow because the input dimension is too high, then using PCA to speed it up can be a reasonable choice. This is probably the most common application of PCA. Another common application of PCA is for data visualization.\n",
    "\n",
    "To understand the value of using PCA for data visualization, the first part of this tutorial post goes over a basic visualization of the IRIS dataset after applying PCA. The second part uses PCA to speed up a machine learning algorithm (logistic regression) on the MNIST dataset.\n",
    "\n",
    "With that, let’s get started! If you get lost, I recommend opening the [video](https://www.youtube.com/watch?v=kApPBm1YsqU) below in a separate tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "# https://www.youtube.com/watch?v=kApPBm1YsqU\n",
    "YouTubeVideo('kApPBm1YsqU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used in this tutorial is from:\n",
    "\n",
    "- [PCA for Data Visualization](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_Data_Visualization_Iris_Dataset_Blog.ipynb)\n",
    "\n",
    "- [PCA to Speed-up Machine Learning Algorithms](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_to_Speed-up_Machine_Learning_Algorithms.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Data Visualization\n",
    "For a lot of machine learning applications it helps to be able to visualize your data. Visualizing `2` or `3` dimensional data is not that challenging. However, even the Iris dataset used in this part of the tutorial is `4` dimensional. You can use PCA to reduce that `4` dimensional data into `2` or `3` dimensions so that you can plot and hopefully understand the data better.\n",
    "\n",
    "__NOTE__: Of course you can do dimension reduction using feature selection - but that is not the point in this tutorial.\n",
    "\n",
    "### Load Iris Dataset\n",
    "The Iris dataset is one of datasets `scikit-learn` comes with that do not require the downloading of any file from some external website. The code below will load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame of given iris dataset.\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the Data\n",
    "\n",
    "PCA is effected by scale so you need to scale the features in your data before applying PCA. Use `StandardScaler` to help you standardize the dataset’s features onto unit scale (`mean` = `0` and `variance` = `1`) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = data.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = data.loc[:,['species']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://cdn-images-1.medium.com/max/640/1*Qxyo-uDrmsUzdxIe7Nnsmg.png' />\n",
    "                            The features in `x` before and after scaling/standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Projection to 2D\n",
    "The original data has `4` columns (`sepal length`, `sepal width`, `petal length`, and `petal width`). In this section, the code projects the original data which is `4` dimensional into `2` dimensions. I should note that after dimensionality reduction, there usually isn’t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://cdn-images-1.medium.com/max/640/1*7jUCr36YguAMKNHTN4Gt8A.png' />\n",
    "PCA with Top 2 Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, data[['species']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating DataFrame along `axis = 1`. `finalDf` is the final DataFrame before plotting the data.\n",
    "<img src = 'https://cdn-images-1.medium.com/max/640/1*4Q1kH0zKeHrnHF7Eg_yhTQ.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 2D Projection\n",
    "This section is just plotting 2 dimensional data. Notice on the graph below that the classes seem well separated from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['species'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance - How we measure if a PCA is good or not\n",
    "The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert `4` dimensional space to `2` dimensional space, you lose some of the variance (information) when you do this. By using the attribute `explained_variance_ratio_`, you can see that the first principal component contains `72.77%` of the variance and the second principal component contains `23.03%` of the variance. Together, the two components contain `95.80%` of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to Speed-up Machine Learning Algorithms\n",
    "One of the most important applications of PCA is for speeding up machine learning algorithms. Using the IRIS dataset would be impractical here as the dataset only has `150` rows and only `4` feature columns. The MNIST database of handwritten digits is more suitable as it has `784` feature columns (784 dimensions), a training set of `60,000` examples, and a test set of `10,000` examples.\n",
    "\n",
    "### Download and Load the Data\n",
    "You can also add a `data_home` parameter to `fetch_mldata` to change where you download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "#my_data_home = './data'\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "\n",
    "print(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images that you downloaded are contained in mnist.data and has a shape of `(70000, 784)` meaning there are 70,000 images with `784` dimensions (`784` features).\n",
    "\n",
    "The labels (the integers `0`–`9`) are contained in mnist.target. The features are 784 dimensional (`28` x `28` images) and the labels are simply numbers from `0`–`9`.\n",
    "\n",
    "### Split Data into Training and Test Sets\n",
    "Typically the train test split is `80%` training and `20%` test. In this case, I chose `6/7th` of the data to be training and `1/7`th of the data to be in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=1/7.0, random_state=2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the Data\n",
    "The text in this paragraph is almost an exact copy of what was written earlier. PCA is effected by scale so you need to scale the features in the data before applying PCA. You can transform the data onto unit scale (mean = `0` and variance = `1`) which is a requirement for the optimal performance of many machine learning algorithms. `StandardScaler` helps standardize the dataset’s features. Note you fit on the training set and transform on the training and test set. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the [effects of not standardizing your data](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Apply PCA\n",
    "Notice the code below has `.95` for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that `95%` of the variance is retained.\n",
    "\n",
    "__NOTE__: This is a very intelligent way of applying PCA - normally you will need to define the number of PCs and function to generate them - now you can rely on `sklearn` to take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit PCA on training set. \n",
    "\n",
    "__Note__: you are fitting PCA on the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(train_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: You can find out how many components PCA choose after fitting the model using `pca.n_components_`. In this case, `95%` of the variance amounts to `327` principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mapping (transform) to both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Machine Learning - Logistic Regression\n",
    "\n",
    "\n",
    "Apply Logistic Regression to the Transformed Data. \n",
    "__Step 1__: Import the model you want to use\n",
    "\n",
    "In `sklearn`, all machine learning models are implemented as Python classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: Make an instance of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow which is why it was changed to 'lbfgs'\n",
    "lr = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: Training the model on the data, storing the information learned from the data\n",
    "\n",
    "Model is learning the relationship between digits and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(train_img, train_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: Predict the labels of new data (new images)\n",
    "\n",
    "Uses the information the model learned during the model training process.\n",
    "\n",
    "The code below predicts for __one__ observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation (image)\n",
    "lr.predict(test_img[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below predicts for multiple observations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation (image)\n",
    "lr.predict(test_img[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "While **accuracy** is not always the best metric for machine learning algorithms (precision, recall, F1 Score, [ROC Curve](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0), etc would be better), it is used here for simplicity.\n",
    "\n",
    "__NOTE__: One reason we do not use the other metrics is because this is a **multi-class classification** problem. The other metrics listed above are more proper, or can only be used to **binary classification** problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_img, test_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, huh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing of Fitting Logistic Regression after PCA\n",
    "\n",
    "The whole point of this section of the tutorial was to show that you can use PCA to speed up the fitting of machine learning algorithms. The table below shows how long it took to fit logistic regression on my MacBook after using PCA (retaining different amounts of variance each time).\n",
    "\n",
    "<img src = 'https://cdn-images-1.medium.com/max/640/1*xKUK0wLnLHAJYS1zbt-7wA.png' />\n",
    "\n",
    "You can observe that with less variance retained in your data, the shorter training time is. Also note that we did not always lose model performance (i.e. __accuracy__) while applying PCA. In this case, the best performing model is the one with 95% accuracy retained.\n",
    "\n",
    "## Closing Thoughts\n",
    "This is a post that I could have written on for a lot longer as PCA has many different uses. I hope this post helps you with whatever you are working on. PCA is a very useful __dimension reduction__ technique. The only drawback I can think of is that PCA is transforming data in a linear fashion - but that can be a problem since an ocean of data follow non-linear fashions. That is why we use advanced techniques such as [autoencoders](https://www.datacamp.com/community/tutorials/autoencoder-classifier-python) for the same dimension reduction purposes. If you are interested in Autoencoders, please contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
